# -*- coding: utf-8 -*-
"""InterestRatePred.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n_-vuWfG8laqeAByqKN9gVfi_kzqCySC

## Importing and understanding the dataset
"""  

!pip -q install kaggle

from google.colab import files
files.upload()  # choose kaggle.json

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle --version  # quick sanity check

from google.colab import drive
drive.mount('/content/drive')    

# choose a download directory (Drive or local)
DOWNLOAD_DIR = '/content/drive/MyDrive/kaggle-data'  # persistent
# DOWNLOAD_DIR = '/content/data'  # <- use this instead if you don't want Drive
import os, pathlib
pathlib.Path(DOWNLOAD_DIR).mkdir(parents=True, exist_ok=True)
print('Saving to:', DOWNLOAD_DIR)

DATASET_SLUG = 'adarshsng/lending-club-loan-data-csv'  # <- change me if needed

# Download and unzip straight into DOWNLOAD_DIR
!kaggle datasets download -d {DATASET_SLUG} -p "{DOWNLOAD_DIR}" --unzip

import os

for root, dirs, files in os.walk('.'):
    for f in files:
        print(os.path.join(root, f))

"""Dataset Importing is complete, now loading the master df."""

import pandas as pd

df = pd.read_csv('./loan.csv')
df.shape  # Check dataset dimensions

print("Grade column exists:", 'grade' in df.columns)
print("Unique grades:", df['grade'].unique() if 'grade' in df.columns else "Missing")

df.head(10)

df.isnull().sum()

"""The dataset has huge amount of features, a detailed study of them is needed. Then only relevant features have to be kept and all others are to be ignored.

# **EDA**
"""

with pd.option_context('display.max_rows', None):
    feature_dtypes = df.dtypes.reset_index()
    feature_dtypes.columns = ["Feature", "DataType"]
    print(feature_dtypes)

"""Some columns had dtype conflict, converting to string for convinience."""

problematic_cols = [19,47,55,112,123,124,125,128,129,130,133,139,140,141]
dtype={col: str for col in problematic_cols}

df.shape          # Rows, columns
df.info()         # Data types & non-null counts
df.describe()     # Basic statistics for numeric columns
  # For categorical columns

with pd.option_context('display.max_columns', None):
    display(df.describe(include='object'))

"""## **INSIGHTS**
1] Term has only 2 types, 36 months and 60 months<br>
2] The grades are primarily 7 and there exist 5 subgrades within each, implying 35 grade types<br>
3] There are more (31-120) days lates than (16-30) days<br>
4] pymnt plans is of two types n and y with huge disparity <br>

"""

df['pymnt_plan'].value_counts()

"""Checking nulls now"""

with pd.option_context('display.max_rows', None):
    null_counts = df.isnull().sum()
    null_percentages = (null_counts / len(df)) * 100
    display(null_percentages.sort_values(ascending=False))

"""# **WHAT IS DROPPED AND WHY? ⛔**

Dropping the entirely null columns, no value added by them to us.
"""

drop_cols = [
    'id', 'member_id', 'url', 'desc'
]
df.drop(columns=drop_cols, inplace=True, errors='ignore')

"""# Hardship-Related Columns (~99%):

hardship_amount

hardship_status

hardship_start_date

hardship_end_date

hardship_length

hardship_reason

hardship_type

hardship_last_payment_amount

hardship_payoff_balance_amount

hardship_loan_status

hardship_dpd

payment_plan_start_date

*These are rarest of rare case hence putting them into the model isnt something that would work. Overfitting would happen, hencing dropping them.*

# Debt Settlement Columns (~98%)

debt_settlement_flag_date

settlement_status

settlement_date

settlement_amount

settlement_percentage

settlement_term

*A debt settlment is a post loan operation, its relevance at time of applying and predicting interest rate is close to zero. Hence to be better dropped.*

# Secondary Applicant Columns (~95%)

sec_app_earliest_cr_line

sec_app_inq_last_6mths

sec_app_mort_acc

sec_app_open_acc

sec_app_open_act_il

sec_app_num_rev_accts

sec_app_revol_util

sec_app_chargeoff_within_12_mths

sec_app_collections_12_mths_ex_med

sec_app_mths_since_last_major_derog

revol_bal_joint

annual_inc_joint

dti_joint

verification_status_joint

*Joint applicant is a rare case and for initial modelling isnt a good idea to keep. Hence dropping them*
"""

# List of columns to drop
cols_to_drop = [
    # Hardship-Related Columns (~99%)
    'hardship_amount',
    'hardship_status',
    'hardship_start_date',
    'hardship_end_date',
    'hardship_length',
    'hardship_reason',
    'hardship_type',
    'hardship_last_payment_amount',
    'hardship_payoff_balance_amount',
    'hardship_loan_status',
    'hardship_dpd',
    'payment_plan_start_date',

    # Debt Settlement Columns (~98%)
    'debt_settlement_flag_date',
    'settlement_status',
    'settlement_date',
    'settlement_amount',
    'settlement_percentage',
    'settlement_term',

    # Secondary Applicant Columns (~95%)
    'sec_app_earliest_cr_line',
    'sec_app_inq_last_6mths',
    'sec_app_mort_acc',
    'sec_app_open_acc',
    'sec_app_open_act_il',
    'sec_app_num_rev_accts',
    'sec_app_revol_util',
    'sec_app_chargeoff_within_12_mths',
    'sec_app_collections_12_mths_ex_med',
    'sec_app_mths_since_last_major_derog',
    'revol_bal_joint',

    'dti_joint',

    'orig_projected_additional_accrued_interest',

    'deferral_term'
]

# Drop these columns safely
df = df.drop(columns=cols_to_drop, errors='ignore')

print(f"Dropped {len(cols_to_drop)} columns. New shape: {df.shape}")

"""# **WHAT I HAVE DECIDED TO KEEP AND WHY? ✅**

mths_since_last_record (~84%)

mths_since_recent_bc_dlq (~77%)

mths_since_last_major_derog (~74%)

annual_inc_joint	(~94%)

verification_status_joint	(~94%)

il_util	(~47%)

mths_since_rcnt_il	(~40%)

all_util	(~38%)

inq_last_12m	(~38%)

total_bal_il	(~38%)

max_bal_bc	(~38%)

*All these have high null percentages but they are crucial factors in a real life lending scenario hence i have kept them.*

## Handling missing values in the retained **columns**
"""

derog_cols = [
    'mths_since_last_record',
    'mths_since_recent_bc_dlq',
    'mths_since_last_major_derog'
]
df[derog_cols] = df[derog_cols].fillna(0)

"""In derogatory columns if there is nothing it indicated no such incident happened. Hence filling them with zeroes."""

df['annual_inc_joint'] = df['annual_inc_joint'].fillna(0)
df['verification_status_joint'] = df['verification_status_joint'].fillna('Not Applicable')

"""If there is not info in a joint section, single borrower case is assumed. Therefore replacing them with apprpriate value types."""

util_cols = ['il_util', 'all_util', 'total_bal_il', 'max_bal_bc']
for col in util_cols:
    df[col] = df[col].fillna(df[col].median())

"""Filled the util columns with median value as if replaced with 0 could increase bias of the dataset."""

df['mths_since_rcnt_il'] = df['mths_since_rcnt_il'].fillna(999)
df['inq_last_12m'] = df['inq_last_12m'].fillna(0)

"""if there is no given months since recent il then it is replaced by very high value to show that it is long pending. If inq_last_12m isnt given it is assumed no inquiery was made, hence replaced with 0."""

with pd.option_context('display.max_rows', None):
    null_counts = df.isnull().sum()
    null_percentages = (null_counts / len(df)) * 100
    display(null_percentages.sort_values(ascending=False))

date_cols = ['next_pymnt_d', 'last_pymnt_d', 'earliest_cr_line', 'last_credit_pull_d']
for col in date_cols:
    df[col] = pd.to_datetime(df[col], errors='coerce')

df['next_pymnt_d_missing'] = df['next_pymnt_d'].isnull().astype(int)
df['next_pymnt_d'] = df['next_pymnt_d'].fillna(df['next_pymnt_d'].max())

months_cols = [ 'mths_since_recent_inq', 'mths_since_rcnt_il', 'mths_since_recent_revol_delinq',
                'mths_since_last_delinq', 'mths_since_last_record', 'mths_since_recent_bc',
                'mths_since_recent_bc_dlq', 'mths_since_last_major_derog' ]

for col in months_cols:
    df[col] = df[col].fillna(0)

numeric_cols = [ 'bc_util', 'revol_util', 'dti', 'inq_fi', 'inq_last_12m', 'tot_cur_bal',
                'total_rev_hi_lim', 'avg_cur_bal', 'total_bc_limit', 'total_bal_ex_mort',
                'total_il_high_credit_limit', 'tot_hi_cred_lim', 'open_il_12m', 'open_il_24m',
                'open_act_il', 'num_rev_accts', 'num_bc_sats', 'num_sats', 'mo_sin_old_il_acct',
                'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl',
                'percent_bc_gt_75', 'pct_tl_nvr_dlq' ]

for col in numeric_cols:
    df[col] = df[col].fillna(df[col].median())

df['emp_title'] = df['emp_title'].fillna("Unknown")
df['title'] = df['title'].fillna("Unknown")
df['zip_code'] = df['zip_code'].fillna(df['zip_code'].mode()[0])

binary_cols = ['collections_12_mths_ex_med', 'chargeoff_within_12_mths', 'tax_liens',
               'pub_rec_bankruptcies', 'pub_rec', 'acc_now_delinq', 'delinq_amnt',
               'inq_last_6mths', 'delinq_2yrs']

for col in binary_cols:
    df[col] = df[col].fillna(0)

missing_after = df.isnull().mean().sort_values(ascending=False)
print(missing_after[missing_after > 0])

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(12,6))
sns.barplot(x=missing_after.index, y=missing_after.values*100)
plt.xticks(rotation=90)
plt.ylabel("Missing Percentage (%)")
plt.title("Missing Values After Imputation")
plt.show()

import numpy as np
import pandas as pd

# 1. Columns to fill with median
median_cols = [
    'total_cu_tl', 'open_acc_6m', 'open_rv_12m', 'open_rv_24m',
    'bc_open_to_buy', 'tot_coll_amt', 'num_actv_bc_tl', 'num_op_rev_tl',
    'num_il_tl', 'num_actv_rev_tl', 'num_rev_tl_bal_gt_0', 'num_tl_op_past_12m',
    'num_accts_ever_120_pd', 'num_bc_tl', 'mort_acc', 'acc_open_past_24mths',
    'open_acc', 'total_acc', 'annual_inc'
]
for col in median_cols:
    df[col] = df[col].fillna(df[col].median())

# 2. Columns to fill with zero (missing = "no such event")
zero_cols = ['num_tl_120dpd_2m', 'num_tl_90g_dpd_24m', 'num_tl_30dpd']
for col in zero_cols:
    df[col] = df[col].fillna(0)

# 3. Categorical columns → use mode
df['emp_length'] = df['emp_length'].fillna(df['emp_length'].mode()[0])

# 4. Date columns → use mode (most common date)
date_cols = ['last_pymnt_d', 'last_credit_pull_d', 'earliest_cr_line']
for col in date_cols:
    df[col] = pd.to_datetime(df[col], errors='coerce')
    if df[col].isnull().sum() > 0:
        df[col] = df[col].fillna(df[col].mode()[0])

# 5. Verify missingness after imputation
missing_after = df[median_cols + zero_cols + ['emp_length'] + date_cols].isnull().mean().sort_values(ascending=False)
print("Remaining missing values:\n", missing_after[missing_after > 0])

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12,6))
sns.barplot(x=missing_after.index, y=missing_after.values*100)
plt.xticks(rotation=90)
plt.ylabel("Missing Percentage (%)")
plt.title("Missing Values After Imputation")
plt.show()



"""## **Our final goal is interest rates prediction, so understanding it:[link text]**"""

import matplotlib.pyplot as plt

# Plot histogram for interest rate distribution
plt.figure(figsize=(10, 6))
plt.hist(df['int_rate'], bins=30, edgecolor='black')
plt.xlabel("Interest Rate (%)", fontsize=12)
plt.ylabel("Frequency", fontsize=12)
plt.title("Distribution of Interest Rates", fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

plt.figure(figsize=(10,6))
sns.scatterplot(x='loan_amnt', y='int_rate', data=df, alpha=0.5)
plt.xlabel("Loan Amount")
plt.ylabel("Interest Rate (%)")
plt.title("Loan Amount vs Interest Rate")
plt.show()

plt.figure(figsize=(8,5))
sns.boxplot(x='grade', y='int_rate', data=df)
plt.xlabel("Grade")
plt.ylabel("Interest Rate (%)")
plt.title("Interest Rate by Grade")
plt.show()

print("Shape of dataset:", df.shape)
print("\nTotal Missing Values:", df.isnull().sum().sum())
print("\nTop 5 rows:")
display(df.head())

# Convert date columns to datetime
date_cols = ['earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d']
for col in date_cols:
    if col in df.columns:
        df[col] = pd.to_datetime(df[col], errors='coerce')

# Convert categorical features to 'category' dtype
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
df[categorical_cols] = df[categorical_cols].astype('category')

# Verify
print("Categorical columns:", categorical_cols)
print("Numeric columns:", df.select_dtypes(include=['int64','float64']).columns.tolist())

with pd.option_context('display.max_rows', None):
    feature_dtypes = df.dtypes.reset_index()
    feature_dtypes.columns = ["Feature", "DataType"]
    print(feature_dtypes)

df['grade'].unique()

import numpy as np

#Handling serious outlier
col = "mths_since_rcnt_il"
if col in df.columns:
    # Find a safe cap value at 95th percentile
    cap_value = df[col].dropna().quantile(0.95)
    df[col] = df[col].fillna(cap_value)

#Dropping post loan processes.
post_loan_cols = [
    "total_pymnt", "total_pymnt_inv", "total_rec_prncp", "total_rec_int",
    "recoveries", "collection_recovery_fee", "last_pymnt_d",
    "last_pymnt_amnt", "next_pymnt_d", "next_pymnt_d_missing",
    "out_prncp", "out_prncp_inv"
]

df = df.drop(columns=[col for col in post_loan_cols if col in df.columns])

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ==============================================================================
# PART 1: DATA LOADING AND INITIAL CLEANING (Based on your EDA)
# ==============================================================================
print("--- Part 1: Loading and Initial Cleaning ---")

# Load the dataset
# Using low_memory=False helps prevent DtypeWarning for this large, mixed-type dataset
try:
    df = pd.read_csv('loan.csv', low_memory=False)
    print(f"Successfully loaded loan.csv. Initial shape: {df.shape}")
except FileNotFoundError:
    print("Error: 'loan.csv' not found. Please ensure the file is in the correct directory.")
    exit()

import matplotlib.pyplot as plt
import seaborn as sns
if 'sub_grade' in df.columns:
    # Sort the sub_grades for a clean, ordered plot
    sub_grade_order = sorted(df['sub_grade'].unique())

    plt.figure(figsize=(16, 8))
    sns.boxplot(x='sub_grade', y='int_rate', data=df, order=sub_grade_order, palette='viridis')
    plt.title('Interest Rate vs. Loan Sub-Grade', fontsize=16)
    plt.xlabel('Loan Sub-Grade', fontsize=12)
    plt.ylabel('Interest Rate (%)', fontsize=12)
    plt.xticks(rotation=45)
    plt.show()
else:
    print("Warning: 'sub_grade' column not found for visualization.")

# ==============================================================================
# 2. Fixed: Purpose of the Loans Visualization
# ==============================================================================
# Check if 'purpose' column exists before plotting
if 'purpose' in df.columns:
    plt.figure(figsize=(14, 7))
    sns.countplot(y='purpose', data=df, order=df['purpose'].value_counts().index, palette='magma')
    plt.title('Count of Loans by Purpose', fontsize=16)
    plt.xlabel('Count', fontsize=12)
    plt.ylabel('Loan Purpose', fontsize=12)
    plt.show()
else:
    print("Warning: 'purpose' column not found for visualization.")

# Display all the unique values in the 'addr_state' column
print(sorted(df['addr_state'].unique()))

import plotly.express as px

# ==============================================================================
# 3. Fixed: Geographical Analysis (Interactive Map)
# ==============================================================================
# Check if 'addr_state' column exists before plotting
if 'addr_state' in df.columns:
    # Group data by state and calculate the average interest rate
    state_avg_rate = df.groupby('addr_state')['int_rate'].mean().reset_index()

    # Create the interactive choropleth map
    fig = px.choropleth(
        state_avg_rate,
        locations='addr_state',
        locationmode="USA-states",
        color='int_rate',
        scope="usa",
        color_continuous_scale="YlOrRd",
        title="Average Loan Interest Rate by State",
        labels={'int_rate': 'Avg. Interest Rate (%)'}
    )
    fig.show()
else:
    print("Warning: 'addr_state' column not found for visualization.")

# --- CORRECTED DROP LIST ---
# 'emp_title' is REMOVED from this initial list.
# We also keep 'installment' and 'sub_grade' out to fix the data leakage.
cols_to_drop = [
    # Identifiers & Leaking Features
    'id', 'member_id', 'url', 'desc', 'installment', 'sub_grade',
    # Hardship-Related Columns
    'hardship_amount', 'hardship_status', 'hardship_start_date', 'hardship_end_date',
    'hardship_length', 'hardship_reason', 'hardship_type', 'hardship_last_payment_amount',
    'hardship_payoff_balance_amount', 'hardship_loan_status', 'hardship_dpd',
    'payment_plan_start_date',
    # Debt Settlement Columns
    'debt_settlement_flag_date', 'settlement_status', 'settlement_date',
    'settlement_amount', 'settlement_percentage', 'settlement_term',
    # Secondary Applicant Columns
    'sec_app_earliest_cr_line', 'sec_app_inq_last_6mths', 'sec_app_mort_acc',
    'sec_app_open_acc', 'sec_app_open_act_il', 'sec_app_num_rev_accts',
    'sec_app_revol_util', 'sec_app_chargeoff_within_12_mths',
    'sec_app_collections_12_mths_ex_med', 'sec_app_mths_since_last_major_derog',
    'revol_bal_joint', 'annual_inc_joint', 'dti_joint', 'verification_status_joint',
    'orig_projected_additional_accrued_interest', 'deferral_term',
    # Post-loan feature columns (leakage)
    "total_pymnt", "total_pymnt_inv", "total_rec_prncp", "total_rec_int",
    "recoveries", "collection_recovery_fee", "last_pymnt_d",
    "last_pymnt_amnt", "next_pymnt_d", "last_credit_pull_d",

    "out_prncp", "out_prncp_inv"
]
df.drop(columns=cols_to_drop, inplace=True, errors='ignore')
print(f"Dropped {len(cols_to_drop)} columns. New shape: {df.shape}")

# ==============================================================================
# PART 2: MISSING VALUE IMPUTATION (This will now work correctly)
# ==============================================================================
print("\n--- Part 2: Imputing Missing Values ---")

# Impute derogatory columns
derog_cols = ['mths_since_last_record', 'mths_since_recent_bc_dlq', 'mths_since_last_major_derog', 'mths_since_last_delinq']
for col in derog_cols:
    if col in df.columns:
        df[col] = df[col].fillna(0)
print("Filled derogatory columns with 0.")

# Impute employment info (This line will no longer cause an error)
df['emp_title'] = df['emp_title'].fillna("Unknown")
df['emp_length'] = df['emp_length'].fillna(df['emp_length'].mode()[0])
print("Filled missing employment info.")

# Impute remaining numerical and object columns
numeric_cols_with_nulls = df.select_dtypes(include=np.number).isnull().sum()
numeric_cols_with_nulls = numeric_cols_with_nulls[numeric_cols_with_nulls > 0].index.tolist()
for col in numeric_cols_with_nulls:
    df[col] = df[col].fillna(df[col].median())
print(f"Filled {len(numeric_cols_with_nulls)} numeric columns with their median.")

object_cols_with_nulls = df.select_dtypes(include=['object', 'category']).isnull().sum()
object_cols_with_nulls = object_cols_with_nulls[object_cols_with_nulls > 0].index.tolist()
for col in object_cols_with_nulls:
    df[col] = df[col].fillna(df[col].mode()[0])

print("No missing values remain.")

# ==============================================================================
# PART 3: FEATURE ENGINEERING & FINAL PREPARATION
# ==============================================================================
print("\n--- Part 3: Feature Engineering and Final Preparation ---")

# Clean target variable and key features
df['int_rate'] = df['int_rate'].str.strip('%').astype(float)
df['term'] = df['term'].str.strip().str.replace(' months', '').astype(int)

# Engineer emp_length
emp_length_mapping = {'< 1 year': 0, '1 year': 1, '2 years': 2, '3 years': 3, '4 years': 4, '5 years': 5, '6 years': 6, '7 years': 7, '8 years': 8, '9 years': 9, '10+ years': 10, 'n/a': 0}
df['emp_length'] = df['emp_length'].map(emp_length_mapping)

# Engineer date features
df['issue_d'] = pd.to_datetime(df['issue_d'], errors='coerce')
df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], errors='coerce')
df['credit_history_length'] = (df['issue_d'] - df['earliest_cr_line']).dt.days
df['credit_history_length'].fillna(df['credit_history_length'].median(), inplace=True)

# --- FINAL DROP LIST ---
# Now is the correct time to drop emp_title, along with other unneeded columns
cols_to_drop_final = ['issue_d', 'earliest_cr_line', 'grade', 'title', 'zip_code', 'emp_title']
df.drop(columns=cols_to_drop_final, inplace=True, errors='ignore')
print("Dropped final set of columns including 'emp_title'.")

# One-hot encode remaining categorical variables
categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True, dtype=int)
print(f"One-hot encoding complete. Final shape: {df.shape}")

print("\n--- Part 4: Splitting and Scaling Data ---")

# 4a. Separate features (X) and target (y)
y = df['int_rate']
X = df.drop(columns='int_rate')
print(f"Shape of X (features): {X.shape}, Shape of y (target): {y.shape}")

# 4b. Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}")

# 4c. Scale numerical features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert scaled arrays back to DataFrames for clarity
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)
print("Feature scaling complete.")
print("\nSample of scaled training data:")
print(X_train_scaled.head())

# ==============================================================================
# FINAL OUTPUT: Your data is ready!
# ==============================================================================
print("\n\n✅ Data preparation complete!")
print("You can now use the following variables for model training:")
print(f" - X_train_scaled: (Shape: {X_train_scaled.shape})")
print(f" - y_train: (Shape: {y_train.shape})")
print(f" - X_test_scaled: (Shape: {X_test_scaled.shape})")
print(f" - y_test: (Shape: {y_test.shape})")

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Set a visually appealing style for the plots
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 7)

# Plotting the distribution of the interest rate
plt.figure(figsize=(12, 6))
sns.histplot(df['int_rate'], kde=True, bins=40, color='blue')
plt.title('Distribution of Interest Rates', fontsize=16)
plt.xlabel('Interest Rate (%)', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.show()

plt.figure(figsize=(12, 7))
sns.jointplot(x='loan_amnt', y='int_rate', data=df, kind='hex', gridsize=50, cmap='inferno')
plt.suptitle('Density of Interest Rate vs. Loan Amount', y=1.02, fontsize=16)
plt.show()

plt.figure(figsize=(8, 6))
sns.boxplot(x='term', y='int_rate', data=df, palette='coolwarm')
plt.title('Interest Rate vs. Loan Term', fontsize=16)
plt.xlabel('Loan Term (Months)', fontsize=12)
plt.ylabel('Interest Rate (%)', fontsize=12)
plt.show()

# Select a subset of important numerical columns for a cleaner heatmap
corr_cols = [
    'int_rate', 'loan_amnt', 'funded_amnt', 'term', 'installment',
    'emp_length', 'annual_inc', 'dti', 'open_acc', 'pub_rec',
    'revol_bal', 'revol_util', 'total_acc', 'credit_history_length'
]

# Calculate the correlation matrix
corr_matrix = df[corr_cols].corr()

plt.figure(figsize=(15, 12))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)
plt.title('Correlation Matrix of Key Numerical Features', fontsize=16)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# --- IMPORTANT ---
# Run this code on your 'df' DataFrame *after* all cleaning and feature engineering,
# but *before* splitting into X/y and scaling.

# ==============================================================================
# Method 1: Visual Heatmap of Key Numerical Features
# ==============================================================================
print("--- Method 1: Visual Heatmap of Key Features ---")

# Select a subset of the most important numerical columns for a readable heatmap.
# These are features that existed before one-hot encoding.
key_features = [
    'int_rate', 'loan_amnt', 'funded_amnt', 'term', 'installment',
    'emp_length', 'annual_inc', 'dti', 'open_acc', 'pub_rec',
    'revol_bal', 'revol_util', 'total_acc', 'credit_history_length',
    'fico_range_high', 'fico_range_low'
]

# Create a subset DataFrame and calculate its correlation matrix
# We check if the column exists in the df before including it
key_features_exist = [col for col in key_features if col in df.columns]
corr_matrix = df[key_features_exist].corr()

# Set up the matplotlib figure
plt.figure(figsize=(16, 12))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(
    corr_matrix,
    annot=True,          # Write the data value in each cell
    fmt='.2f',           # Use two decimal places
    cmap='coolwarm',     # A diverging colormap (red for positive, blue for negative)
    linewidths=.5,       # Add lines between cells
    annot_kws={"size": 10} # Adjust font size of annotations
)

plt.title('Correlation Matrix of Key Numerical Features', fontsize=18)
plt.show()


# ==============================================================================
# Method 2: Top Features Correlated with Interest Rate
# ==============================================================================
print("\n\n--- Method 2: Top Features Correlated with 'int_rate' ---")

# Calculate the correlation of all features with the target variable 'int_rate'
# We use the full DataFrame here to see the impact of one-hot encoded features too.
full_corr_with_target = df.corr()['int_rate'].sort_values(ascending=False)

# Remove the self-correlation (int_rate with itself)
full_corr_with_target = full_corr_with_target.drop('int_rate')

print("--- Top 15 POSITIVELY Correlated Features with Interest Rate ---")
print(full_corr_with_target.head(15))

print("\n\n--- Top 15 NEGATIVELY Correlated Features with Interest Rate ---")
print(full_corr_with_target.tail(15))

# Check for any other columns with zero variance and drop them
cols_to_drop = X_train.columns[X_train.nunique() <= 1]
X_train = X_train.drop(columns=cols_to_drop)
X_test = X_test.drop(columns=cols_to_drop)

print(f"Dropped {len(cols_to_drop)} columns with no variance: {list(cols_to_drop)}")

"""# LINEAR REGRES"""

leak_cols=['id', 'member_id', 'url', 'desc', 'installment', 'sub_grade']
df.drop(columns=leak_cols, inplace=True, errors='ignore')

# Assuming X_train, X_test, y_train, y_test are already created and scaled data is available
# For clarity, let's work with the scaled DataFrames
X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)


# Check for any columns with zero variance (constant value) in the training set
cols_to_drop = X_train_scaled_df.columns[X_train_scaled_df.nunique() <= 1]

if not cols_to_drop.empty:
    print(f"Dropping {len(cols_to_drop)} constant columns: {list(cols_to_drop)}")
    X_train_final = X_train_scaled_df.drop(columns=cols_to_drop)
    X_test_final = X_test_scaled_df.drop(columns=cols_to_drop)
else:
    print("No constant columns to drop. Proceeding with all features.")
    X_train_final = X_train_scaled_df
    X_test_final = X_test_scaled_df

from sklearn.linear_model import LinearRegression
import time # To time our model training

# --- Train the Model ---
print("\n--- Training Linear Regression Model ---")
# Instantiate the model
lr_model = LinearRegression()

# Start the timer
start_time = time.time()

# Train the model using the scaled training data
lr_model.fit(X_train_final, y_train)

# Stop the timer
end_time = time.time()
lr_training_time = end_time - start_time
print(f"Training completed in {lr_training_time:.2f} seconds.")

# --- Make Predictions ---
print("Making predictions on the test set...")
lr_preds = lr_model.predict(X_test_final)

# (Assuming the evaluate_model function from our previous discussion is available)
# If not, here it is again:
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def evaluate_model(y_true, y_pred, model_name):
    """Calculates and prints regression metrics."""
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)

    print(f"\n--- {model_name} Performance ---")
    print(f"R-squared (R²): {r2:.4f}")
    print(f"Mean Absolute Error (MAE): {mae:.4f} (Avg. prediction is off by {mae:.2f} percentage points)")
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
    print("-" * 35)
    return {'R2': r2, 'MAE': mae, 'RMSE': rmse}

# --- Evaluate the Model ---
lr_performance = evaluate_model(y_test, lr_preds, "Linear Regression")

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 10))
sns.scatterplot(x=y_test, y=lr_preds, alpha=0.3)
# Plot the "perfect prediction" line
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.title('Actual vs. Predicted Interest Rates (Linear Regression)', fontsize=16)
plt.xlabel('Actual Interest Rate (%)', fontsize=12)
plt.ylabel('Predicted Interest Rate (%)', fontsize=12)
plt.axis('equal') # Ensure the x and y axes have the same scale
plt.grid(True)
plt.show()

# Calculate residuals
residuals = y_test - lr_preds

plt.figure(figsize=(12, 6))
sns.scatterplot(x=lr_preds, y=residuals, alpha=0.3)
# Plot the zero error line
plt.axhline(y=0, color='r', linestyle='--')
plt.title('Residuals vs. Predicted Values (Linear Regression)', fontsize=16)
plt.xlabel('Predicted Interest Rate (%)', fontsize=12)
plt.ylabel('Residuals (Actual - Predicted)', fontsize=12)
plt.grid(True)
plt.show()

# Create a dictionary to store our results
model_performance = {}

model_performance['Linear Regression'] = {
    'R2': lr_performance['R2'],
    'MAE': lr_performance['MAE'],
    'RMSE': lr_performance['RMSE'],
    'Training Time (s)': lr_training_time
}

# Display the performance so far in a DataFrame
performance_df = pd.DataFrame.from_dict(model_performance, orient='index')
print("\n--- Model Performance Summary ---")
print(performance_df)

